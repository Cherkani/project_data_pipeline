# Analyse des CorrÃ©lations Crypto-Stock avec Architecture Modulaire

## ğŸš€ Description

Ce projet implÃ©mente une infrastructure modulaire pour analyser et visualiser les corrÃ©lations entre les cryptomonnaies (Bitcoin, Ethereum) et les actions Nvidia. Il combine des flux de donnÃ©es en temps rÃ©el et des donnÃ©es historiques pour fournir une analyse approfondie et une visualisation interactive via des tableaux de bord.

---

## ğŸ“Š FonctionnalitÃ©s Principales

- **Acquisition de donnÃ©es en temps rÃ©el** depuis les APIs Binance et Finnhub.

- **Stockage** des donnÃ©es en temps rÃ©el avec Kafka et des donnÃ©es historiques avec Elasticsearch.

- **Traitement des donnÃ©es** avec Apache Flink/Spark pour calculer des corrÃ©lations.

- **Visualisation avancÃ©e** avec Tableau, permettant une analyse approfondie des tendances historiques et des relations en temps rÃ©el.

---

## ğŸ— Architecture

L'architecture repose sur une approche modulaire avec les composants suivants :

- **NiFi** : RÃ©cupÃ©ration des donnÃ©es des APIs.

- **Kafka** : Gestion des flux de donnÃ©es en temps rÃ©el.

- **Elasticsearch** : Stockage et interrogation des donnÃ©es.

- **Apache Flink/Spark** : Traitement des donnÃ©es en streaming.

- **Tableau** : CrÃ©ation de graphiques interactifs et dynamiques.

---

## âš™ï¸ Ã‰tapes de Lancement du Projet

### Chapitre 1 : Clonage et Lancement du Projet

1. **Cloner le projet :**
   ```bash
   git clone https://github.com/Cherkani/project_data_pipeline.git
   ```


Cela tÃ©lÃ©chargera une copie du projet localement.

2. AccÃ©der au rÃ©pertoire :

  ```bash
  cd project_data_pipeline
  ```

3. DÃ©marrer les services avec Docker Compose :

```bash
docker-compose up
```

Cette commande initialise les conteneurs Docker nÃ©cessaires et garantit que les dÃ©pendances (Kafka, Elasticsearch, etc.) sont opÃ©rationnelles.


## Configurer le Flux de DonnÃ©es avec Apache NiFi
1. AccÃ©dez Ã  l'interface NiFi et configurez les flux suivants :

  - InvokeHTTP : Configurez des requÃªtes pour collecter les donnÃ©es depuis les APIs (Binance et Finnhub).
  
  - GenerateFlowFile : Synchronisez les donnÃ©es collectÃ©es.
  
  - PublishKafka_2_6 : Publiez les donnÃ©es dans des topics Kafka.
  
  - Configurez les URLs des APIs :

2. Binance API : RÃ©cupÃ¨re les prix des cryptomonnaies.
```bash
https://api.binance.com/api/v3/ticker/price?symbols=%5B%22BTCUSDT%22,%22ETHUSDT%22%5D
```

Finnhub API : RÃ©cupÃ¨re les donnÃ©es Nvidia.

```bash
https://finnhub.io/api/v1/quote?symbol=NVDA&token=<votre_token>
```

## Gestion des Topics Kafka

1. Connectez-vous Ã  l'interface Kafka.

2. CrÃ©ez les topics nÃ©cessaires :

- my_topic

- topic2

Configurez les partitions et le facteur de rÃ©plication selon vos besoins.

Traitement des DonnÃ©es avec Apache Flink
Compilez et empaquetez le projet avec Maven :

```bash
mvn clean package
```

Ajoutez un job dans l'interface Flink :

Chargez le fichier JAR gÃ©nÃ©rÃ©.
Configurez la classe principale et soumettez le job.
Analyse avec Apache Spark




1. DÃ©ployer le script Spark : TransfÃ©rez le fichier Spark vers le conteneur :

```bash
docker cp /path/to/script.py spark-master:/opt/bitnami/spark/
```


2. ExÃ©cuter le script Spark :

```bash
spark-submit /opt/bitnami/spark/script.py
```

3. VÃ©rifiez lâ€™Ã©tat des jobs dans lâ€™interface Spark.

## IntÃ©gration avec Elasticsearch et Kibana

1. Assurez-vous qu'Elasticsearch est opÃ©rationnel.

2. AccÃ©dez Ã  Kibana via l'URL configurÃ©e et ouvrez l'onglet Discover.

3. Recherchez les indices (par exemple, btc, eth) pour analyser les donnÃ©es en temps rÃ©el.

4. RÃ©pÃ©tez ces Ã©tapes pour chaque index utilisÃ© afin de valider les enregistrements.



## Visualisation avec Tableau

1. Configurez une connexion avec Elasticsearch dans Tableau en utilisant l'URL suivante :

```bash
http://localhost:9200
```

2. CrÃ©ez des graphiques interactifs avec les donnÃ©es des indices (btc, eth, nvda).

ğŸ“„ Structure du Projet

- docker-compose.yml : DÃ©finit tous les services nÃ©cessaires.
- nifi/ : Flux NiFi pour collecter et publier les donnÃ©es.
- spark/ : Scripts pour le traitement en temps rÃ©el avec Spark.
- dashboards/ : ModÃ¨les de tableau de bord pour Tableau.

ğŸŒ APIs UtilisÃ©es

- Binance API : Prix des cryptomonnaies en temps rÃ©el.

- Finnhub API : DonnÃ©es des actions Nvidia.

ğŸ“Š Tableaux de Bord

Les tableaux de bord incluent :

- Variations des prix BTC, ETH, NVDA.

- CorrÃ©lations entre cryptomonnaies et actions.

- Visualisation des tendances historiques.


ğŸ›¡ Contributions

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez le dÃ©pÃ´t.
2. CrÃ©ez une branche pour vos modifications.
3. Soumettez une Pull Request.




















# Analyse des CorrÃ©lations Crypto-Stock avec Architecture Modulaire

## ğŸš€ Description


Ce projet implÃ©mente une infrastructure modulaire pour analyser et visualiser les corrÃ©lations entre les cryptomonnaies (Bitcoin, Ethereum) et les actions Nvidia. Il combine des flux de donnÃ©es en temps rÃ©el et des donnÃ©es historiques pour fournir une analyse approfondie et une visualisation interactive via des tableaux de bord.


# ğŸ“Š FonctionnalitÃ©s Principales

## Acquisition de donnÃ©es en temps rÃ©el depuis les APIs Binance et Finnhub.

Stockage des donnÃ©es en temps rÃ©el avec Kafka et des donnÃ©es historiques avec Elasticsearch.

Traitement des donnÃ©es avec Apache Flink/Spark pour calculer des corrÃ©lations.

Visualisation avancÃ©e avec Tableau, permettant une analyse approfondie des tendances historiques et des relations en temps rÃ©el.

# video

https://github.com/user-attachments/assets/92fd0eba-f6c6-42ab-ad52-1b28de5eedba


# ğŸ— Architecture

![data pipeline](https://github.com/user-attachments/assets/3f7eea15-9a67-4aa8-b661-d22235c440d4)


## L'architecture repose sur une approche modulaire avec les composants suivants :

NiFi : RÃ©cupÃ©ration des donnÃ©es des APIs.

Kafka : Gestion des flux de donnÃ©es en temps rÃ©el.

Elasticsearch : Stockage et interrogation des donnÃ©es.

Apache Flink/Spark : Traitement des donnÃ©es en streaming.

Tableau : CrÃ©ation de graphiques interactifs et dynamiques.

# ğŸ“ˆ Pipeline de Traitement

## Collecte des donnÃ©es : API Binance (BTC, ETH) et Finnhub (NVDA).

Streaming en temps rÃ©el : Kafka.

Analyse : Flink/Spark calcule les relations entre les cryptomonnaies et Nvidia.

Visualisation : Tableau affiche les donnÃ©es traitÃ©es Ã  partir de lâ€™index Elasticsearch.


# ğŸ›  PrÃ©requis

## Avant de commencer, assurez-vous d'avoir les outils suivants installÃ©s :

ğŸ³ Docker Desktop

ğŸ Python 3.8+

ğŸ’» 16 Go de RAM (minimum recommandÃ©)

ğŸ–¥ï¸ Git

ğŸ“Š Tableau




# âš™ï¸ Installation et DÃ©marrage
## 1ï¸âƒ£ Cloner le DÃ©pÃ´t

git clone git clone [project_data_pipeline](https://github.com/Cherkani/project_data_pipeline.git)

cd project_data_pipeline

## 2ï¸âƒ£ Lancer l'Infrastructure

Assurez-vous que Docker est en cours d'exÃ©cution, puis exÃ©cutez :

docker-compose up --build

## 3ï¸âƒ£ Configurer le Tableau de Bord dans Tableau

Connexion aux DonnÃ©es :

Configurez une connexion dans Tableau avec Elasticsearch en utilisant l'adresse suivante :
arduino

http://localhost:9200

CrÃ©er des Graphiques :

Configurez des visualisations interactives en utilisant les index des donnÃ©es (btc, eth, nvda).

## ğŸ“„ Structure du Projet

docker-compose.yml : DÃ©finit tous les services nÃ©cessaires (Kafka, Elasticsearch, NiFi, etc.).

nifi/ : Flux NiFi pour collecter et publier les donnÃ©es.

spark/ : Scripts pour le traitement en temps rÃ©el avec Spark.

dashboards/ : ModÃ¨les de tableau de bord pour Tableau.

## ğŸŒ APIs UtilisÃ©es

Binance API : Prix des cryptomonnaies en temps rÃ©el.

Finnhub API : DonnÃ©es des actions Nvidia.

## ğŸ“Š Tableaux de Bord

Les tableaux de bord incluent :

Variations des prix BTC, ETH, NVDA.

CorrÃ©lations entre cryptomonnaies et actions.

Visualisation des tendances historiques.

## ğŸ›¡ Contributions

Les contributions sont les bienvenues ! Pour contribuer :


