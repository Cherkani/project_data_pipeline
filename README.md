# Analyse des Corr√©lations Crypto-Stock avec Architecture Modulaire

## üöÄ Description

Ce projet impl√©mente une infrastructure modulaire pour analyser et visualiser les corr√©lations entre les cryptomonnaies (Bitcoin, Ethereum) et les actions Nvidia. Il combine des flux de donn√©es en temps r√©el et des donn√©es historiques pour fournir une analyse approfondie et une visualisation interactive via des tableaux de bord.

---

## üìä Fonctionnalit√©s Principales

- **Acquisition de donn√©es en temps r√©el** depuis les APIs Binance et Finnhub.

- **Stockage** des donn√©es en temps r√©el avec Kafka et des donn√©es historiques avec Elasticsearch.

- **Traitement des donn√©es** avec Apache Flink/Spark pour calculer des corr√©lations.

- **Visualisation avanc√©e** avec Tableau, permettant une analyse approfondie des tendances historiques et des relations en temps r√©el.

---

## üèó Architecture



L'architecture repose sur une approche modulaire avec les composants suivants :

- **NiFi** : R√©cup√©ration des donn√©es des APIs.

- **Kafka** : Gestion des flux de donn√©es en temps r√©el.

- **Elasticsearch** : Stockage et interrogation des donn√©es.

- **Apache Flink/Spark** : Traitement des donn√©es en streaming.

- **Tableau** : Cr√©ation de graphiques interactifs et dynamiques.

---

## ‚öôÔ∏è √âtapes de Lancement du Projet

### Chapitre 1 : Clonage et Lancement du Projet

1. **Cloner le projet :**

   
```bash
git clone https://github.com/Cherkani/project_data_pipeline.git
```


Cela t√©l√©chargera une copie du projet localement.

2. Acc√©der au r√©pertoire :

  ```bash
  cd project_data_pipeline
  ```

3. D√©marrer les services avec Docker Compose :

```bash
docker-compose up
```

Cette commande initialise les conteneurs Docker n√©cessaires et garantit que les d√©pendances (Kafka, Elasticsearch, etc.) sont op√©rationnelles.


## Configurer le Flux de Donn√©es avec Apache NiFi
1. Acc√©dez √† l'interface NiFi et configurez les flux suivants :

  - InvokeHTTP : Configurez des requ√™tes pour collecter les donn√©es depuis les APIs (Binance et Finnhub).
  
  - GenerateFlowFile : Synchronisez les donn√©es collect√©es.
  
  - PublishKafka_2_6 : Publiez les donn√©es dans des topics Kafka.
  
  - Configurez les URLs des APIs :

2. Binance API : R√©cup√®re les prix des cryptomonnaies.
```bash
https://api.binance.com/api/v3/ticker/price?symbols=%5B%22BTCUSDT%22,%22ETHUSDT%22%5D
```

Finnhub API : R√©cup√®re les donn√©es Nvidia.

```bash
https://finnhub.io/api/v1/quote?symbol=NVDA&token=<votre_token>
```

## Gestion des Topics Kafka

1. Connectez-vous √† l'interface Kafka.

2. Cr√©ez les topics n√©cessaires :

- my_topic

- topic2

Configurez les partitions et le facteur de r√©plication selon vos besoins.

Traitement des Donn√©es avec Apache Flink
Compilez et empaquetez le projet avec Maven :

```bash
mvn clean package
```

Ajoutez un job dans l'interface Flink :

Chargez le fichier JAR g√©n√©r√©.
Configurez la classe principale et soumettez le job.
Analyse avec Apache Spark




1. D√©ployer le script Spark : Transf√©rez le fichier Spark vers le conteneur :

```bash
docker cp /path/to/script.py spark-master:/opt/bitnami/spark/
```


2. Ex√©cuter le script Spark :

```bash
spark-submit /opt/bitnami/spark/script.py
```

3. V√©rifiez l‚Äô√©tat des jobs dans l‚Äôinterface Spark.

## Int√©gration avec Elasticsearch et Kibana

1. Assurez-vous qu'Elasticsearch est op√©rationnel.

2. Acc√©dez √† Kibana via l'URL configur√©e et ouvrez l'onglet Discover.

3. Recherchez les indices (par exemple, btc, eth) pour analyser les donn√©es en temps r√©el.

4. R√©p√©tez ces √©tapes pour chaque index utilis√© afin de valider les enregistrements.



## Visualisation avec Tableau

1. Configurez une connexion avec Elasticsearch dans Tableau en utilisant l'URL suivante :

```bash
http://localhost:9200
```

2. Cr√©ez des graphiques interactifs avec les donn√©es des indices (btc, eth, nvda).

üìÑ Structure du Projet

- docker-compose.yml : D√©finit tous les services n√©cessaires.
- nifi/ : Flux NiFi pour collecter et publier les donn√©es.
- spark/ : Scripts pour le traitement en temps r√©el avec Spark.
- dashboards/ : Mod√®les de tableau de bord pour Tableau.

üåê APIs Utilis√©es

- Binance API : Prix des cryptomonnaies en temps r√©el.

- Finnhub API : Donn√©es des actions Nvidia.

üìä Tableaux de Bord

Les tableaux de bord incluent :

- Variations des prix BTC, ETH, NVDA.

- Corr√©lations entre cryptomonnaies et actions.

- Visualisation des tendances historiques.


üõ° Contributions

Les contributions sont les bienvenues ! Pour contribuer :

1. Forkez le d√©p√¥t.
2. Cr√©ez une branche pour vos modifications.
3. Soumettez une Pull Request.






## Video



https://github.com/user-attachments/assets/27146820-8a39-408c-b72c-94e8a0995e7a













